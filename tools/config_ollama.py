#!/usr/bin/env python3

import subprocess
import re
import time
from ollama import chat

#============================================
def get_vram_size_in_gb():
	"""
	Detects the available VRAM or unified memory in GB.

	Returns:
		int: VRAM size in GB.
	"""
	try:
		architecture = subprocess.check_output(["uname", "-m"], text=True).strip()
		is_apple_silicon = architecture.startswith("arm64")

		if is_apple_silicon:
			hardware_info = subprocess.check_output(["system_profiler", "SPHardwareDataType"], text=True)
			memory_match = re.search(r"Memory:\s(\d+)\s?GB", hardware_info)
			if memory_match:
				return int(memory_match.group(1))
		else:
			display_info = subprocess.check_output(["system_profiler", "SPDisplaysDataType"], text=True)
			vram_match = re.search(r"VRAM.*?: (\d+)\s?MB", display_info)
			if vram_match:
				vram_in_mb = int(vram_match.group(1))
				return vram_in_mb // 1024  # Convert MB to GB

	except Exception as e:
		print(f"Error getting memory info: {e}")

	return None

#============================================
# Select Ollama Model Based on VRAM
vram_size_gb = get_vram_size_in_gb()
MODEL_NAME = "llama3.2:1b-instruct-q4_K_M"  # Default

if vram_size_gb:
	if vram_size_gb > 30:
		MODEL_NAME = "phi4:14b-q8_0"
	elif vram_size_gb > 14:
		MODEL_NAME = "phi4:14b-q4_K_M"
	elif vram_size_gb > 4:
		MODEL_NAME = "llama3.2:3b-instruct-q5_K_M"

print(f"Selected Ollama model: {MODEL_NAME}")

#============================================
#============================================
def unit_test():
	"""
	Simple unit test for LLM function.
	"""
	print("Running unit test...")
	import random
	# Generate a random math problem
	num1 = random.randint(10, 99)
	num2 = random.randint(10, 99)
	expected_answer = num1 + num2
	# Create a prompt
	prompt = f"What is {num1} + {num2}? "
	prompt += "Provide just the answer in plain text."
	# Get response from LLM
	response = run_ollama(prompt)
	# Attempt to parse the response
	try:
		ai_answer = int(response)
	except ValueError:
		print(f"FAILED: Response was not a valid number â†’ {response}")
	if ai_answer == expected_answer:
		print(f"SUCCESS! {num1} + {num2} = {ai_answer}")
	else:
		print(f"FAILED: Expected {expected_answer}, but got {ai_answer}")


#============================================
def run_ollama(prompt: str, model: str = MODEL_NAME) -> str:
	"""
	Generate a response using the Ollama model.

	Args:
		prompt (str): The prompt to send to the model.
		model (str): The name of the Ollama model to use.

	Returns:
		str: The response content generated by the model.
	"""
	t0 = time.time()
	response = chat(
		model=model,
		messages=[{'role': 'user', 'content': prompt}]
	)
	response_content = response['message']['content'].strip()
	print(f"Ollama completed in {time.time()-t0:.2f} seconds")

	return response_content
