#!/usr/bin/env python3

import subprocess
import re
import time
from ollama import chat

#============================================
def is_ollama_running():
	"""
	Checks if the Ollama server is running.
	"""
	try:
		output = subprocess.check_output(["pgrep", "ollama"], text=True).strip()
		if output:
			return True
	except subprocess.CalledProcessError:
		pass
	return False

#============================================
def start_ollama_if_needed():
	"""
	Checks if Ollama is running and provides instructions if it's not.
	"""
	if not is_ollama_running():
		print("\n⚠Ollama is not running! Start it with: `ollama serve`\n")
		print("Attempting to start Ollama automatically...")
		subprocess.Popen(["ollama", "serve"])
		time.sleep(3)  # Give it some time to start
		if is_ollama_running():
			print("Ollama started successfully!")
		else:
			print("Failed to start Ollama. Please run `ollama serve` manually.")

#============================================
def get_vram_size_in_gb():
	"""
	Detects the available VRAM or unified memory in GB.

	Returns:
		int: VRAM size in GB.
	"""
	try:
		architecture = subprocess.check_output(["uname", "-m"], text=True).strip()
		is_apple_silicon = architecture.startswith("arm64")

		if is_apple_silicon:
			hardware_info = subprocess.check_output(["system_profiler", "SPHardwareDataType"], text=True)
			memory_match = re.search(r"Memory:\s(\d+)\s?GB", hardware_info)
			if memory_match:
				return int(memory_match.group(1))
		else:
			display_info = subprocess.check_output(["system_profiler", "SPDisplaysDataType"], text=True)
			vram_match = re.search(r"VRAM.*?: (\d+)\s?MB", display_info)
			if vram_match:
				vram_in_mb = int(vram_match.group(1))
				return vram_in_mb // 1024  # Convert MB to GB

	except Exception as e:
		print(f"Error getting memory info: {e}")

	return None

#============================================
def pull_ollama_model(model_name):
	"""
	Automatically pulls the required Ollama model if it is not available.
	"""
	print(f"Checking for model: {model_name}")
	output = subprocess.check_output(["ollama", "list"], text=True)
	if model_name not in output:
		print(f"Model {model_name} not found. Pulling now...")
		subprocess.run(["ollama", "pull", model_name], check=True)
		print(f"Successfully pulled model: {model_name}")


#============================================
# Select Ollama Model Based on VRAM
vram_size_gb = get_vram_size_in_gb()
MODEL_NAME = "llama3.2:1b-instruct-q4_K_M"  # Default

if vram_size_gb:
	if vram_size_gb > 30:
		MODEL_NAME = "phi4:14b-q8_0"
	elif vram_size_gb > 14:
		MODEL_NAME = "phi4:14b-q4_K_M"
	elif vram_size_gb > 4:
		MODEL_NAME = "llama3.2:3b-instruct-q5_K_M"

print(f"Selected Ollama model: {MODEL_NAME}")

# Ensure Ollama is running
start_ollama_if_needed()
# Ensure the model is pulled
pull_ollama_model(MODEL_NAME)

#============================================
def unit_test():
	"""
	Simple unit test for LLM function.
	"""
	print("Running unit test...")
	import random
	# Generate a random math problem
	num1 = random.randint(10, 99)
	num2 = random.randint(10, 99)
	expected_answer = num1 + num2
	# Create a prompt
	prompt = f"What is {num1} + {num2}? "
	prompt += "Provide just the answer in plain text."
	# Get response from LLM
	response = run_ollama(prompt)
	# Attempt to parse the response
	try:
		ai_answer = int(response)
	except ValueError:
		print(f"FAILED: Response was not a valid number → {response}")
	if ai_answer == expected_answer:
		print(f"SUCCESS! {num1} + {num2} = {ai_answer}")
	else:
		print(f"FAILED: Expected {expected_answer}, but got {ai_answer}")

#============================================
def run_ollama(prompt: str, model: str = MODEL_NAME) -> str:
	"""
	Generate a response using the Ollama model.

	Args:
		prompt (str): The prompt to send to the model.
		model (str): The name of the Ollama model to use.

	Returns:
		str: The response content generated by the model.
	"""
	t0 = time.time()
	response = chat(
		model=model,
		messages=[{'role': 'user', 'content': prompt}]
	)
	response_content = response['message']['content'].strip()
	print(f"Ollama completed in {time.time()-t0:.2f} seconds")

	return response_content
